{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QtE9MWO2mnnS"
      },
      "outputs": [],
      "source": [
        "import nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSgiQZCUm31A",
        "outputId": "588a72cb-3396-46df-9c72-aaecd7cc6962"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> Download\n",
            "Command 'Download' unrecognized\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"Hello students, how are you doing today? The olympics are inspiring, and Python is awesome. You look nice today.\"\n",
        "\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "id": "ldjJAgpdm-I3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "id": "LNY2oRKzoSCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(set(stopwords.words('english')))"
      ],
      "metadata": {
        "id": "4T5s3PwaoWdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_sent = \"This is some sample text, showing off the stop words filtration.\"\n",
        "stop_words = set(stopwords.words(english))\n",
        "word_tokens = word_tokenize(example_sent)\n",
        "filtered_sentence = [w for w in word_tenkens if not w in stop_words]\n",
        "filtered_sentence = []\n",
        "\n",
        "for w in word_tokens:\n",
        "  if w not in stop_words:\n",
        "    filtered_sentence.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "id": "556G3c32oea_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming Words With NLTK"
      ],
      "metadata": {
        "id": "FrofSiOLpc9Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "example_words = [\"ride\", \"riding\", \"rider\", \"rides\"]\n",
        "for w in example_words:\n",
        "  print(ps.stem(w))\n",
        ""
      ],
      "metadata": {
        "id": "3_ZTwKb4pcid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets try stemming an entire sentence!The olympics are inspiring, and Python is awesome. You look nice today.\n",
        "new_text = \"When riders are riding their horses, they often think of how cowboys rode horses.\"\n",
        "words = word_tokenize(new_text)\n",
        "for w in words:\n",
        "  print(ps.stem(w))"
      ],
      "metadata": {
        "id": "0ED6LoRVrq6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download()"
      ],
      "metadata": {
        "id": "o8VlXGVTr-mP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can documents from the nltk.corpus. As an example, lets load the universal declaration\n",
        "from nltk.corpus import udhr\n",
        "print(udhr.raw('English-Latin1'))"
      ],
      "metadata": {
        "id": "zyMmDfYYsAs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets import some sample and training text - George Bush's 2005 and 2006 state of the union addresses.\n",
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")"
      ],
      "metadata": {
        "id": "v7QW5Nh5sPOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now that we have some text, we can train the PunkSentenceTokenizer.\n",
        "custom_sent_tokenizer = PunkSentenceTokenizer(train_text)"
      ],
      "metadata": {
        "id": "EE_TPQH8sujM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This function will tag each tokenized word with a part of speech\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized[:5]:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "      print(tagged)\n",
        "\n",
        "    except Exception as e:\n",
        "      print(str(e))\n",
        "\n",
        "\n",
        "process_content()\n"
      ],
      "metadata": {
        "id": "IjacHlYis7O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking With NLTK"
      ],
      "metadata": {
        "id": "zz7PFLTHtitb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "+ = match 1 or more\n",
        "? = match 0 or 1 repetitions.\n",
        "* = match 0 or MORE repetitions\n",
        ". = Any character except a new line\n",
        "'''"
      ],
      "metadata": {
        "id": "aCmWrWjitgUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
        "\n",
        "custom_sent_tokenizer = PunkSentenceTokenizer(train_text)\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "\n",
        "      # combine the part of the speech tag with a regular expression\n",
        "\n",
        "      chunkGram = r\"\"\"Chunk:{<RB.?>*<VB.?>*<NNP>+<NN??}\"\"\"\n",
        "      chunkParser = nltk.RegexParser(chunkGram)\n",
        "      chunked = chunkParser.parse(tagged)\n",
        "\n",
        "      #draw the chunks with nltk\n",
        "      #chunked.draw()\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "process_content()"
      ],
      "metadata": {
        "id": "GsoPmHHmtqHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main line in question is:"
      ],
      "metadata": {
        "id": "O82F0FtBuypx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "'''"
      ],
      "metadata": {
        "id": "rcUtb7UGux7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line, broken down"
      ],
      "metadata": {
        "id": "AGMB_tX6u3Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "<RB.?>* = \"0 or more of any tense of adverb,\" followed by:\n",
        "\n",
        "<VB.?>* = \"0 or more of any tense of verb,\" followed by:\n",
        "\n",
        "<NNP>+ = \"One or more proper nouns,\" followed by\n",
        "\n",
        "<NN>? = \"zero or one singular noun.\"\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "DmawR48eu1uh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "\n",
        "      # combine the part of the speech tag with regular expression\n",
        "\n",
        "      chunkGram = r\"\"\"Chunk: {<RB.?>*<VB.?>*<NNP>+<NN>?}\"\"\"\n",
        "      chunkParser = nltk.RegexpParser(chunkGram)\n",
        "      chunked = chunkParser.parse(tagged)\n",
        "\n",
        "      for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk')\n",
        "        print(subtree)\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "process_content()"
      ],
      "metadata": {
        "id": "sMLWbzEju8w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "\n",
        "      chunkGram = r\"\"\"Chunk: {<.*>+}\n",
        "                                    }<VB.?|IN|DT|TO>+{\"\"\"\n",
        "\n",
        "      chunkParser = nltk.RegexpParser(chunkGram)\n",
        "      chunked = chunkParser.parse(tagged)\n",
        "\n",
        "      for subtree in chunked.subtree(filter= lambda t: t.label() =='Chunk')\n",
        "        print(subtree)\n",
        "\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "\n",
        "process_content()\n"
      ],
      "metadata": {
        "id": "1_pjyAlCvhoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_content():\n",
        "    try:\n",
        "        for i in tokenized[5:]:\n",
        "            words = nltk.word_tokenize(i)\n",
        "            tagged = nltk.pos_tag(words)\n",
        "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
        "            # namedEnt.draw()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(str(e))\n",
        "\n",
        "\n",
        "process_content()"
      ],
      "metadata": {
        "id": "RWb3gCIqwEa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Classification Using Natural Language Processing"
      ],
      "metadata": {
        "id": "Ryd_zBUawO2n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qzBvVUwKwO0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "\n",
        "documents = [(list(movie_reviews.words(fileid)), category)\n",
        "for categroy in movie_reviews.categories()\n",
        "for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# shuffle the documents\n",
        "random.shuffle(documents)\n",
        "\n",
        "print('Number of Documents: {}'.format(len(documents)))\n",
        "print('First Review: {}'.format(documents[1]))\n",
        "\n",
        "all_words = []\n",
        "for w in movie_reviews.words():\n",
        "  all_words.append(w.lower())\n",
        "\n",
        "all_words = nltk.FreqDist(all_words)\n",
        "\n",
        "print('Most common words: {}'.format(all_words.most_common(15)))\n",
        "print('The word happy: {}'.format(all_words[\"happy\"]))\n"
      ],
      "metadata": {
        "id": "nkcLpMrLwJTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll use the 4000 most common words as features\n",
        "print(len(all_words))\n",
        "word_features = list(all_words.keys())[:4000]"
      ],
      "metadata": {
        "id": "IlUKVa8JxboF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The find_features funciton will determine which of the 3000 word features are contained in the review\n",
        "def find_features(document):\n",
        "  words = set(document)\n",
        "  features = {}\n",
        "  for w in word_features:\n",
        "    features[w] = (w in words)\n",
        "\n",
        "  return features\n",
        "\n",
        "\n",
        "features = find_features(movie_reviews.words('neg/cv000_29416.txt'))\n",
        "for key, value in features.items():\n",
        "  if value ==True:\n",
        "    print(key)"
      ],
      "metadata": {
        "id": "AxKT7hz4xpaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now lets do it for all the documents\n",
        "featuresets = [(find_features(rev), category) for (rev, category) in documents]\n"
      ],
      "metadata": {
        "id": "FEvFL0hCybyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import model_selection\n",
        "\n",
        "# define a seed for reproducibility\n",
        "seed = 1\n",
        "\n",
        "# split the data into training and testing datasets\n",
        "training, testing = model_selection.train_test_split(featuresets, test_size=0.25, random_state=seed)"
      ],
      "metadata": {
        "id": "NmB-uavWyo7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(training))\n",
        "print(len(testing))"
      ],
      "metadata": {
        "id": "MeeKTnUDy_ao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use sklearn algorithms in NLTK\n",
        "from nltk.classify.scikitlearn import SklearnClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SklearnClassifer(SVC(kernel= 'linear'))\n",
        "#train the model\n",
        "model.train(training)\n",
        "\n",
        "#and test on the testset\n",
        "accuracy = nltk.classify.accuracy(model, testing)*100\n",
        "print(\"SVC Accuracy: {}\".format(accuracy))"
      ],
      "metadata": {
        "id": "XswfwENOzFPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}